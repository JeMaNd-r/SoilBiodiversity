---
title: "Soil Biodiversity SDMs"
author: "Romy Zeiss"
date: "12/3/2021"
output: 
  html_document:
      toc: TRUE
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(here)

library(CoordinateCleaner)
library(rgbif)
library(taxize)

library(raster)
library(sp)
library(extremevalues)
library(sdmpredictors)

library(biomod2) # also to create pseudo-absences
#library(snowfall) # for parallelization
#library(SSDM)

library(usdm) # for variable inflation factor calculation

library(mgcv) # for GAM
library(gam)  # for GLM (!)
library(remotes) #to download package from github

## for regularized regressions
# installing the package from github
remotes::install_github("rvalavi/myspatial")
library(glmnet)
library(myspatial)

library(caret) # for MARS and BRT
library(earth) # for MARS
library(doParallel) # for MARS and XGBoost

library(dismo) # for MaxEnt and BRT
# download maxent.jar 3.3.3k, and place the file in the
# desired folder
utils::download.file(url = "https://raw.githubusercontent.com/mrmaxent/Maxent/master/ArchivedReleases/3.3.3k/maxent.jar", 
    destfile = paste0(system.file("java", package = "dismo"), 
        "/maxent.jar"), mode = "wb")  ## wb for binary file, otherwise maxent.jar can not execute

library(maxnet) # for MaxNet

library(xgboost) # for XGBoost
library(randomForest) # for RF
library(e1071) # for SVM

library(ROCR) # for Ensemble model
library(scales) # for Ensemble model

# for model performance:
library(precrec)
library(ggplot2) # for plotting the curves
# devtools::install_github("meeliskull/prg/R_package/prg")
library(prg)
library(ggpubr)
library (ROCR)
library(sdm) # to calculate kappa

## different input data required and package quite newly released...:
#remotes::install_github("peterbat1/fitMaxnet")
#library(fitMaxnet) #for variable importance of maxnet object

# plotting
library(GGally) #for correlations with ggpairs

here::here()
```

Note: The command `here::here()` should give us the folder called *SoilBiodiversity*.

This document works as a pipeline document from which several scripts will be runned. The general structure follows the table of content (see also Fig. 1).


```{r workflow, echo=F}
DiagrammeR::grViz("digraph {
  graph [layout = dot, rankdir = TB]
  
  node [shape = rectangle]        
  rec1 [label = 'Step 1. Biodiversity Data']
  rec2 [label = 'Step 2. Environmental Data']
  rec3 [label =  'Step 3. Species Distribution Models (SDMs)']
  rec4 [label = 'Step 4. Diversity Maps']
  
  # edge definitions with the node IDs
  rec1 -> rec3;
  rec2 -> rec3;
  rec3 -> rec4
  }",
  height = 500)
```
Figure 1: Workflow to create Species Distribution Models (SDMs) of several soil taxa.

First, we need to define some fixed parameters.

```{r parameters, echo=TRUE}
# Name of the taxon we are interested in
Taxon_name <- "Crassiclitellata"

# Type of rank of the Taxon_name
Taxon_rank <- "order"

# geographic extent
Search_polygon <- "POLYGON((-28.94532 66.07036, -23.32032 29.18799, 38.55468 31.61348, 51.91406 43.80071, 56.83593 62.08195, 44.17968 72.69797, 7.61718 74.29467, -28.94532 66.07036))" 

# create vector graph object called Border_EU for plotting
Border_EU <- borders(database="world",colour = "black",fill = "gray50",  
                 xlim = c(-23, 60), 
                 ylim = c(31, 75))

# geographic extent of Europe
extent_Europe <- c(-23, 60, 31, 75)

# number of records that will be downloaded (max. 100,000)
No_records <- 100000

# do we want to download (new) occurrence records?
checkDownload_rawData <- FALSE

# do we want to save the cleaned data? If so, say TRUE
checkSave_cleanData <- TRUE

# do we want to save the cleaned environmental data?
checkSave_precitor <- FALSE

# define the species you wanted names
#speciesNames <- read.csv(file=paste0(here::here(), "/data/Species_list_", Taxon_name, ".csv"))    #pre-defined list only
speciesNames <- read.csv(file=paste0(here::here(), "/results/Species_list_", Taxon_name, ".csv")) #number of records added

# define names of predictor variables
predictorNames <- c("bio1", "bio2", "bio3", "bio4", "bio5", "bio6", "bio7", "bio8", "bio9", "bio10", "bio11", "bio12", "bio13", "bio14", "bio15", "bio16", "bio17", "bio18", "bio19" , "Latitude")

# define first species
spID <- speciesNames[73, "SpeciesID"]
```


# Biodiversity data

First, we have to get the biodiversity data that is, occurrence data on diverse soil taxa. Second, we need to get all data into the same data format, before we can merge all taxa together in a third step.

## Download

We use different data sources to download occurrence data. Same have to be downloaded manually.

### Download using "taxize" package

The *taxize* package is searching in several databases for a specific taxon name, and simultaneously checks for spelling mistakes/ typos.

We first run the script to download and clean the data. We use the package CoordinateCleaner to identify occurrence records with geospatial issues. We also remove records collected before 1990. 

Note: If we download all available data (or the maximum limit of 100,000), it will take a while. If GBIF has more than 100,000 occurrence records, we need to download the data in several steps.

```{r downloadTaxize, include=FALSE}
```

```{r cleanOcc}
source(file=paste0(here::here(),"Clean_occurrences.R"))
```


For the order Crassiclitellata, we have many occurrences in the Netherlands (countryCode="NL"), but most of them have a too high coordinateUncertainty (more than 1km), and/or are not at the species level, but at the family level only. They were therefore excluded and appear as a "red cloud" on the map above.

### Download from GBIF

We additionally download data from GBIF based on a pre-defined species list that contains only soil-living taxa (whenever it was possible to separate them from aquatic taxa). From the taxa in the species list, we first identify the GBIF taxonomic key, before we download all data based on these keys.

```{r downloadGBIF, include=F}
source(file=paste0(here::here(),"/src/Download_occurrences.R"))
```


## Harmonization

To get all biodiversity data into the same format, we reduce available occurrence data to presence/absence per grid cell. The grid we use has a resolution of 1kmÂ².

```{r harmonization}
source(file=paste0(here::here(),"src/Harmonize_occurrences.R"))
```

## Merging

We merge occurrences of taxa belonging to the same group of organism into one table and merge with the gridcell identifiers (GridID). That way it will be easier to build the ensemble models later on.

First, we create again a dataframe with the pre-defined structure for the data. Then, we add each species as a column to that dataframe.

```{r mergeSpecies}
source(file=paste0(here::here(),"src/Occurrences_to_grid.R"))
```

## Quick visualization

```{r numberOcc}
# load species list containing information on number of occurrence records
species.list <- read.csv(file=paste0(here::here(), "/results/Species_list_", Taxon_name, ".csv"))

# plot some of the species' occurrences
hist(species.list$NumCells)
```

```{r plotOcc}
# load species list containing information on number of occurrence records
occ.points <- read.csv(file=paste0(here::here(), "/results/Occurrence_rasterized_", Taxon_name, ".csv"))

# load background map
#...

# plot total species' occurrences
plotOcc <- ggplot(occ.points %>% group_by(Latitude=round(x,0), Longitude=round(y,0)) %>% 
                    count(name="Number of Records"), 
       aes(x=Latitude, y=Longitude, color=`Number of Records`))+ #, alpha=`Number of Records`
  #geom_polygon(data=bg.map)
  geom_point()+
   scale_color_gradientn(    # provide any number of colors
    colors = c("black", "blue", "orange"),
    values = scales::rescale(c(1, 5, 20, 30, 50, 100, 300)), 
    breaks = c(5, 20, 50, 100, 200))+
  theme_bw()+
  theme(legend.position = "bottom")

plotOcc

# pdf(paste0(here::here(), "/figures/RawOccurrences_", Taxon_name, ".pdf")); plotOcc; dev.off()
```

Now, we make one plot with raw occurrences per species.

```{r plotOccSpecies}
# calculate individual species' occurrences
occ.points.species <- occ.points %>% 
         pivot_longer(cols=speciesNames$SpeciesID[speciesNames$SpeciesID %in% colnames(occ.points)], 
                      names_to = "SpeciesID") %>% 
        mutate("Latitude"=round(x,0), "Longitude"=round(y,0)) %>%
        group_by(Latitude, Longitude, SpeciesID) %>%
        filter(!is.na(value)) %>%
        summarize("Number of Records"= n(), .groups="keep") %>%
        filter("Number of Records" > 0) 

# only keep species that will be analyzed (i.e., present in at least 5 grid cells)
occ.points.species <- occ.points.species[occ.points.species$SpeciesID %in%
                                           speciesNames[speciesNames$NumCells >=5, "SpeciesID"],]
occ.points.species

# plot some of the individual species' occurrences
plotOccSpecies <- ggplot(occ.points.species, 
       aes(x=Latitude, y=Longitude, color=`Number of Records`, group=SpeciesID))+
  #geom_polygon(data=bg.map)
  geom_point(cex=0.02)+
  facet_wrap(vars(SpeciesID))+
  scale_color_gradientn(    # provide any number of colors
    colors = c("black", "blue", "orange"),
    values = scales::rescale(c(1, 5, 20, 30, 50, 100, 300)), 
    breaks = c(5, 20, 50, 100, 200))+
  
  # add number of grid cells in which the species is present
  geom_text(data=occ.points.species %>% group_by(SpeciesID) %>% summarize("n"=sum(`Number of Records`)), 
             aes(x=50, y=33, label=paste0("n=", n)), color="black", 
            inherit.aes=FALSE, parse=FALSE, cex=0.7)+
  theme_bw()+
  theme(axis.text.x = element_blank(), axis.text.y = element_blank(), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.ticks.length = unit(0, "cm"),
        legend.position = "bottom", legend.text = element_text(size=5))
plotOccSpecies

# pdf(paste0(here::here(), "/figures/RawOccurrences_", Taxon_name, "_perSpecies.pdf")); plotOccSpecies; dev.off()
```


# Environmental data

We selected the environmental variables according to the literature. After loading the data, we check for colinearity between the variables. We remove variables that show colinearity of more than XXX. 

## Download

We download the environmental variables manually. Table 1 shows the respective Reference and links.

```{r tablePredictors}
read.csv(file=paste0(here::here(), "/doc/Env_Predictors.csv")) %>%
  dplyr::select(Category, Predictor, Long_name, Original_name, Unit, Range, Range_EU, Resolution, Projection, Reference, Spatial_scale)
```

: Table 1: Selected environmental variables. CRS - Coordinate Reference System

We may need to extract point-specific (climatic) values.

```{r extractPredictors, include=TRUE}
source(file=paste0(here::here(),"Extract_predictor.R"))
```


After download, we load the data in R, combine them into one table, and check for correlation.

## Harmonization

The environmental variables all come with different resolution and coordinate reference systems. We transform all variables into a 1km x 1km grid with WGS84 as coordinate reference system. 

Because this is computationally intensive - especially in R - we do this in ArcGIS by creating a fishnet grid across Europe, and adding all variables to that grid. We get one table (GridID and variable value) for each variables. 

If we merge these tables in ArcGIS, we will get 0 values instead of NA. Therefore, we decided to merge the individual tables in R in the following.

## Merging

We start with the creation of the preferred output dataframe.

```{r}
```

```{r preparePredictors, echo=F}
df.env <- tibble(GridID=1)[0,]

source(file=paste0(here::here(),"Prepare_predictors.R"))
```

Now, we can add all variables we want to include in our models to this dataframe.


## Check for colinearity

Before we start with the models, we check for colinearity between our environmental variables. Because most of the environmental variables have been calculated and or extrapolated, we can check for their theoretical relationships.

```{r relationshipPredictors}
read.csv("Env_predictors_estimation.csv")
# corrplot or similar plot to show raw correlations
```


```{r colinearityPredictors}
source(file=paste0(here::here(), "Colinearity_predictors.R"))
```


The selected environmental variables will be called **predictors** hereafter. We save the names of those selected variables into a vector for later.

```{r covarsNames}
covarsNames <- predictorNames[predictorNames!="Latitude"] 
```


# Model input

## Background data or pseudo-absences

We create background data (pseudo-absences) for the different modeling approaches. The selected methods to create background data were chosen according to Barbet-Massin et al. (2012).

* large number (e.g. 10,000) of pseudo-absences with 
  * equal weighting for presences and absences when using regression techniques (e.g. generalised linear model and generalised additive model); 
  * averaging several runs (e.g. 10) with fewer pseudo-absences (e.g. 100) with equal weighting for presences and absences with multiple adaptive regression splines and discriminant analyses; 
  * using the same number of pseudo-absences as available presences (averaging several runs if few pseudo-absences) for classification techniques such as boosted regression trees, classification trees and random forest 
* random selection of pseudo-absences when using regression techniques and 
* random selection of geographically and environmentally stratified pseudo-absences when using classification and machine-learning techniques

```{r backgroundData, echo=FALSE}
source(file=paste0(here::here(),"Create_backgroundData.R"))
```


## Merging biodiversity and environmental data

```{r mergeEnvOcc, include=F}
# add environmental data to presence data
env <- read.csv(file=paste0(here::here(), "/results/EnvPredictor_", Taxon_name, ".csv"))
occ <- read.csv(file=paste0(here::here(), "/results/Occurrences_", Taxon_name, ".csv"))
occ.env <- cbind(occ, env)

write.csv(occ.env, file=paste0(here::here(), "/results/OccEnv_", Taxon_name, ".csv"), row.names = F)
```

If you get an error message here regarding the length (nrow) of the dataframes, check if you extracted the environmental values with the selected occurrences.

## Split data into training, testing, and validation data

Split the dataset into training (60%), testing (20%) and validation (20%) data. In the same step, get the data into the right data format.

```{r prepareModelInput}
source(file=paste0(here::here(),"Prepare_input.R"))
```


## Define model parameters

```{r}

```


# Modelling

## run Species Distribution Models (SDMs)

```{r SDMs, include=F}
# decide how many CPU to use for parallelization
setCPU <- 5

source(file=paste0(here::here(), "/src/SDMs.R"))
```

```{r SDM_Valavi, echo=F}
source(file=paste0(here::here(), "/src/SDM_Valavi.R"))
```

Please be aware that the validation data sets are randomly simulated (10 times) for some of the models. They would need to be averaged before use!

## predict SDMS

```{r}
source(file=paste0(here::here(), "/src/Predict_SDMs.R"))
```


# Repeat for all other taxa

## (Earthworms)

```{r}
# for loop has to contain following: for(spID in speciesNames[speciesNames$NumCells >=5,]$SpeciesID){}

# and please! only 10 cores (in SDM_Valavi.R: 2x 2 cores used)
```


## Collembola

## Nematoda

## Fungi

## Bacteria

## Microarthropods (excluding Collembola)


# Make raster files

We want to get raster files in the end to map the distributions. Therefore, we have to transform each prediction dataframe into a raster file.

```{r}
source(file=paste0(here::here(), "/Prediction_to_raster.R"))
```


# Results and Visualization

## Input data

```{r resultInput}
dat_cl <- read.csv(file=paste0(here::here(), "/results/Occurrences_", Taxon_name, ".csv"))

table(dat_cl$species)
```

## Variable correlations

```{r}
# plot VIF

# plot correlations
#GGally::ggpairs()

# plot variable importance (averaged(?))
```


## Individual SDMs

```{r resultSDM, include=F}
source(file=paste0(here::here(),"Result_SDMs.R"))
```


## Diversity per group


```{r resultGroupDiv, include=F}
source(file=paste0(here::here(),"Result_richness_perGroup.R"))
```

## Total diversity

```{r resultDiv, include=F}
source(file=paste0(here::here(),"Result_richness.R"))
```

## Model performance

```{r resultModelPerformance}
source(file=paste0(here::here(),"Model_performance.R"))
```

