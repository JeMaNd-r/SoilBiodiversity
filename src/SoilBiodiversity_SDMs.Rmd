---
title: "Soil Biodiversity SDMs"
author: "Romy Zeiss"
date: "12/3/2021"
output: 
  html_document:
      toc: TRUE
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)

options(java.parameters = c("-XX:+UseConcMarkSweepGC", "-Xmx8g")) # expand Java memory
gc()
library(tidyverse)
library(here)

library(CoordinateCleaner)
library(rgbif)
library(taxize)

library(raster)
library(sp)
library(extremevalues)
library(sdmpredictors)

library(biomod2) # also to create pseudo-absences
#library(snowfall) # for parallelization
#library(SSDM)

library(usdm) # for variable inflation factor calculation

library(mgcv) # for GAM
library(gam)  # for GLM (!)
library(remotes) #to download package from github

## for regularized regressions
# installing the package from github
#remotes::install_github("rvalavi/myspatial")
library(glmnet)
library(myspatial)

library(caret) # for MARS and BRT
library(earth) # for MARS
library(doParallel) # for MARS and XGBoost

library(dismo) # for MaxEnt and BRT
# download maxent.jar 3.3.3k, and place the file in the
# desired folder
utils::download.file(url = "https://raw.githubusercontent.com/mrmaxent/Maxent/master/ArchivedReleases/3.3.3k/maxent.jar", 
    destfile = paste0(system.file("java", package = "dismo"), 
        "/maxent.jar"), mode = "wb")  ## wb for binary file, otherwise maxent.jar can not execute

library(maxnet) # for MaxNet

library(xgboost) # for XGBoost
library(randomForest) # for RF
library(e1071) # for SVM

#library(ROCR) # for Ensemble model
#library(scales) # for Ensemble model

# for model performance:
library(precrec)
library(ggplot2) # for plotting the curves
# devtools::install_github("meeliskull/prg/R_package/prg")
library(prg)
library(ggpubr)
library(ROCR)
library(sdm) # to calculate kappa

## different input data required and package quite newly released...:
#remotes::install_github("peterbat1/fitMaxnet")
#library(fitMaxnet) #for variable importance of maxnet object

# plotting
library(GGally) #for correlations with ggpairs
library(gridExtra)

here::here()

write("TMPDIR = 'D:/00_datasets/Trash'", file=file.path(Sys.getenv('R_USER'), '.Renviron'))

# change temporary directory for files
raster::rasterOptions(tmpdir = "D:/00_datasets/Trash")
```

Note: The command `here::here()` should give us the folder called *SoilBiodiversity*.

This document works as a pipeline document from which several scripts will be runned. The general structure follows the table of content (see also Fig. 1).


```{r workflow, echo=F}
DiagrammeR::grViz("digraph {
  graph [layout = dot, rankdir = TB]
  
  node [shape = rectangle]        
  rec1 [label = 'Step 1. Biodiversity Data']
  rec2 [label = 'Step 2. Environmental Data']
  rec3 [label =  'Step 3. Species Distribution Models (SDMs)']
  rec4 [label = 'Step 4. Diversity Maps']
  rec5 [label = 'Step 5. Evaluation']
  
  # edge definitions with the node IDs
  rec1 -> rec3;
  rec2 -> rec3;
  rec3 -> rec4;
  rec4 -> rec5;
  }",
  height = 500)
```
Figure 1: Workflow to create Species Distribution Models (SDMs) of several soil taxa.

First, we need to define some fixed parameters.

```{r parameters, echo=TRUE}
# Name of the taxon we are interested in
Taxon_name <- "Crassiclitellata"

# Type of rank of the Taxon_name
Taxon_rank <- "order"

# geographic extent
Search_polygon <- "POLYGON((-28.94532 66.07036, -23.32032 29.18799, 38.55468 31.61348, 51.91406 43.80071, 56.83593 62.08195, 44.17968 72.69797, 7.61718 74.29467, -28.94532 66.07036))" 

# create vector graph object called Border_EU for plotting
Border_EU <- borders(database="world",colour = "black",fill = "gray50",  
                 xlim = c(-23, 60), 
                 ylim = c(31, 75))

# geographic extent of Europe
extent_Europe <- c(-23, 60, 31, 75)

# number of records that will be downloaded (max. 100,000)
No_records <- 100000

# do we want to download (new) occurrence records?
checkDownload_rawData <- FALSE

# do we want to save the cleaned data? If so, say TRUE
checkSave_cleanData <- TRUE

# do we want to save the cleaned environmental data?
checkSave_precitor <- FALSE

# define the species you wanted names
speciesNames <- read.csv(file=paste0(here::here(), "/data/Species_list_", Taxon_name, ".csv"))    #pre-defined list only

# define first species
spID <- "Lumbr_terr"
#spID <- "Aporr_cali"
```


Let's check, how many unique species we have in the raw datasets (i.e., including non-European species from Phillips et al. 2019).

```{r}
data.frame("No_species" = length(unique(speciesNames$Species)),
           "No_speciesID" = length(unique(speciesNames$SpeciesID)))
```


# Biodiversity data

First, we have to get the biodiversity data that is, occurrence data on diverse soil taxa. Second, we need to get all data into the same data format, before we can merge all taxa together in a third step.

## Download

We use different data sources to download occurrence data. Same have to be downloaded manually.

We first run the script to download and clean the data. We use the package CoordinateCleaner to identify occurrence records with geospatial issues. We also remove records collected before 1990. 

Note: If we download all available data (or the maximum limit of 100,000), it will take a while. If GBIF has more than 100,000 occurrence records, we need to download the data in several steps.

### Download from GBIF

We additionally download data from GBIF based on a pre-defined species list that contains only soil-living taxa (whenever it was possible to separate them from aquatic taxa). From the taxa in the species list, we first identify the GBIF taxonomic key, before we download all data based on these keys.

```{r downloadGBIF, include=F}
source(file=paste0(here::here(),"/src/Download_occurrences.R"))
```


```{r cleanOcc}
source(file=paste0(here::here(),"Clean_GBIF_occurrences.R"))
```

For the order Crassiclitellata, we have many occurrences in the Netherlands (countryCode="NL"), but most of them have a too high coordinateUncertainty (more than 1km), and/or are not at the species level, but at the family level only. They were therefore excluded and appear as a "red cloud" on the map above.

### Merge datasets 

We will merge the three datasets and use CoordinateCleaner to remove problematic records.

```{r harmonization}
source(file=paste0(here::here(),"src/Prepare_biodiversity_data.R"))
```

## Harmonization

To get all biodiversity data into the same format, we reduce available occurrence data to presence/absence per grid cell. The grid we use has a resolution of 1kmÂ².


```{r mergeSpecies}
source(file=paste0(here::here(),"src/Occurrences_to_1km_grid.R"))
source(file=paste0(here::here(),"src/Occurrences_to_2km_grid.R"))
```

```{r updateSpeciesList}
# load species list containing information on number of occurrence records
speciesNames <- read.csv(file=paste0(here::here(), "/results/Species_list_", Taxon_name, ".csv"))
```

## Quick visualization

```{r numberOcc}
data.frame("No_species" = length(unique(speciesNames[speciesNames$NumCells_2km>=50,"Species"])),
           "No_speciesID" = length(unique(speciesNames[speciesNames$NumCells_2km>=50,"SpeciesID"])))
# >0: 155 (139 IDs), >=5: 81 (74 IDs), >=50: 21 (27 IDs), >=100: 27 (24 IDs), >=200: 18 (16 IDs); >=300: 15 (13 IDs)


# plot some of the species' occurrences
hist(speciesNames$NumCells_1km)
hist(speciesNames$NumCells_2km)
```

The histogram shows us that most species occur in less than 500 grid cells.

## Species correlations

... forgot why I wanted to do this...


# Environmental data

We selected the environmental variables according to the literature. After loading the data, we check for colinearity between the variables. We remove variables that show colinearity of more than XXX. 

## Download

We download the environmental variables manually. Table 1 shows the respective Reference and links.

```{r tablePredictors}
read.csv(file=paste0(here::here(), "/doc/Env_Predictors.csv")) %>%
  dplyr::select(Category, Predictor, Long_name, Original_name, Unit, Range, Range_EU, Resolution, Projection, Reference, Spatial_scale)
```

: Table 1: Selected environmental variables. CRS - Coordinate Reference System

We may need to extract point-specific (climatic) values.

```{r extractPredictors, include=TRUE}
source(file=paste0(here::here(),"Extract_predictor.R"))
```


After download, we load the data in R, combine them into one table, and check for correlation.

## Harmonization

The environmental variables all come with different resolution and coordinate reference systems. We transform all variables into a 1km x 1km grid with WGS84 as coordinate reference system. 

Because this is computationally intensive - especially in R - we do this in ArcGIS by creating a fishnet grid across Europe, and adding all variables to that grid. We get one table (GridID and variable value) for each variables. 

If we merge these tables in ArcGIS, we will get 0 values instead of NA. Therefore, we decided to merge the individual tables in R in the following.

## Merging

We start with the creation of the preferred output dataframe.

```{r}

```

```{r preparePredictors, echo=F}
df.env <- tibble(GridID=1)[0,]

source(file=paste0(here::here(),"Prepare_predictors.R"))
```

Now, we can add all variables we want to include in our models to this dataframe.


## Check for colinearity

Before we start with the models, we check for colinearity between our environmental variables. Because most of the environmental variables have been calculated and or extrapolated, we can check for their theoretical relationships.

```{r relationshipPredictors}
read.csv("Env_predictors_estimation.csv")
# corrplot or similar plot to show raw correlations
```


```{r colinearityPredictors}
source(file=paste0(here::here(), "Colinearity_predictors.R"))
```


The selected environmental variables will be called **predictors** hereafter. We save the names of those selected variables into a vector for later.

```{r covarsNames}
#corMatSpearman <- read.csv(file=paste0(here::here(),"/results/", Taxon_name, "/corMatSpearman_predictors.csv"))
corMatPearson <- as.matrix(read.csv(file=paste0(here::here(),"/results/corMatPearson_predictors.csv")))
dimnames(corMatPearson)[[1]] <- dimnames(corMatPearson)[[2]]

# based on Valavi et al. 2021: Pearson 0.8
env_exclude <- caret::findCorrelation(corMatPearson, cutoff = 0.8, names=TRUE)

covarsNames <- dimnames(corMatPearson)[[1]][!(dimnames(corMatPearson)[[1]] %in% env_exclude)]
covarsNames <- covarsNames[covarsNames != "x" & covarsNames != "y"]

# exclude based on VIF
env_vif <- read.csv(file=paste0(here::here(), "/results/VIF_predictors.csv"))
env_exclude <- env_vif %>% filter(is.na(VIF)) %>% dplyr::select(Variables) %>% as.character()

covarsNames <- covarsNames[!(covarsNames %in% env_exclude)]

# excluded:
print("=== We excluded the following variables based on VIF and Pearson correlation: ===")
setdiff(env_vif$Variables, covarsNames)

# final predictor variables
print("=== And we kept the following, final predictor variables: ===")
covarsNames

# caret::findCorrelation(corMatSpearman, cutoff = 0.8, names=TRUE)
# caret::findCorrelation(corMatSpearman, cutoff = 0.7, names=TRUE)
```


# Model input

## Background data or pseudo-absences

We create background data (pseudo-absences) for the different modeling approaches. The selected methods to create background data were chosen according to Barbet-Massin et al. (2012).

* large number (e.g. 10,000) of pseudo-absences with 
  * equal weighting for presences and absences when using regression techniques (e.g. generalised linear model and generalised additive model); 
  * averaging several runs (e.g. 10) with fewer pseudo-absences (e.g. 100) with equal weighting for presences and absences with multiple adaptive regression splines and discriminant analyses; 
  * using the same number of pseudo-absences as available presences (averaging several runs if few pseudo-absences) for classification techniques such as boosted regression trees, classification trees and random forest 
* random selection of pseudo-absences when using regression techniques and 
* random selection of geographically and environmentally stratified pseudo-absences when using classification and machine-learning techniques

```{r backgroundData, echo=FALSE}
source(file=paste0(here::here(),"Create_backgroundData.R"))
```


## Merging biodiversity and environmental data

```{r mergeEnvOcc, include=F}
# add environmental data to presence data
env <- read.csv(file=paste0(here::here(), "/results/EnvPredictor_", Taxon_name, ".csv"))
occ <- read.csv(file=paste0(here::here(), "/results/Occurrences_", Taxon_name, ".csv"))
occ.env <- cbind(occ, env)

write.csv(occ.env, file=paste0(here::here(), "/results/OccEnv_", Taxon_name, ".csv"), row.names = F)
```

If you get an error message here regarding the length (nrow) of the dataframes, check if you extracted the environmental values with the selected occurrences.

## Split data into training, testing, and validation data

Split the dataset into training (60%), testing (20%) and validation (20%) data. In the same step, get the data into the right data format.

```{r prepareModelInput}
source(file=paste0(here::here(),"Prepare_input.R"))
```


## Define model parameters

```{r}

```


# Modelling

## run Species Distribution Models (SDMs)

```{r SDMs, include=F}
# decide how many CPU to use for parallelization
setCPU <- 5

source(file=paste0(here::here(), "/src/SDMs.R"))
```

```{r SDM_Valavi, echo=F}
source(file=paste0(here::here(), "/src/SDM_Valavi.R"))
```

Please be aware that the validation data sets are randomly simulated (10 times) for some of the models. They would need to be averaged before use!

## predict SDMS

```{r}
source(file=paste0(here::here(), "/src/Predict_SDMs.R"))
```

And finally

## transform prediction to binary

```{r}
source(file=paste0(here::here(), "/src/Estimate_richness.R"))
```



# Repeat for all other taxa

## (Earthworms)

```{r}
# for loop has to contain following: for(spID in speciesNames[speciesNames$NumCells >=5,]$SpeciesID){}

# and please! only 10 cores (in SDM_Valavi.R: 2x 2 cores used)
```


## Collembola

## Nematoda

## Fungi

## Bacteria

## Microarthropods (excluding Collembola)

# Uncertainty and sensitivity analysis

Sensitivity analysis: We will check, if SDMs for species with many occurrence records (i.e., present in >200 grid cells) are similar to the SDMs for the same species made with only a subset of available records (i.e., with only 50, 10, and 5 records).

```{r}
source(file=paste0(here::here(), "/Sensitivity_analysis.R"))
source(file=paste0(here::here(), "/Uncertainty_analysis.R"))
```


# Results and Visualization

## Input data

### Occurrence records

First, number of raw occurrences.

```{r numberOcc}
# load raw data
occ_raw <- read.csv(file=paste0(here::here(), "/data/Earthworm_occurrence_GBIF-sWorm-Edaphobase.csv"))

# load cleaned occurrence records 
occ_clean <- read.csv(file=paste0(here::here(), "/results/Occurrences_", Taxon_name, ".csv"))

# load matrix containing information on number of occurrence records in grid
occ_points <- read.csv(file=paste0(here::here(), "/results/Occurrence_rasterized_1km_", Taxon_name, ".csv"))

# calculate number of records per datasource
full_join(occ_raw %>% group_by(datasource) %>% count(name="raw"), 
          occ_clean %>% group_by(datasource) %>% count(name="clean"))

# load number removed records during cleaning
read.csv(file=paste0(here::here(), "/results/NoRecords_cleaning_", Taxon_name, ".csv"))


# count occurrences per species & data source
count_data <- occ_raw %>% group_by(datasource, species) %>% count() %>%
  pivot_wider(names_from = datasource, values_from = n) %>%
  full_join(occ_clean %>% group_by(datasource, species) %>% count() %>%
  pivot_wider(names_from = datasource, values_from = n), suffix = c("_raw", "_clean"), by="species")
count_data$RawOcc <- rowSums(count_data[,2:4], na.rm=T)
count_data$CleanOcc <- rowSums(count_data[,5:7], na.rm=T)

# add number of records after
count_data <- count_data %>% full_join(speciesNames[,c("Species", "Acc_name", "SpeciesID", "NumCells_1km", "NumCells_2km")], by=c("species" = "Species")) %>%
  filter(!is.na(SpeciesID))

# replace NA with 0
#count_data[is.na(count_data)] <- 0

# add info if species was analysed or not
count_data$Included <- FALSE
count_data[count_data$NumCells_1km >=5, "Included"] <- TRUE 

# sort by included or not, and have a look
count_data <- count_data %>% arrange(desc(Included), species)
count_data

# save
write.csv(count_data, file=paste0(here::here(), "/results/NoRecords_perSpecies_full_", Taxon_name, ".csv"), row.names = F)
```

Add information of functional (ecological) groups, and visualize.

```{r numOccFG}
count_data <- count_data %>% full_join(speciesNames %>%
                                         dplyr::select(-NumCells, - Group_name), by=c("species" = "Species"))
count_data

ggplot(count_data %>% filter(!is.na(SpeciesID), Included=TRUE), aes(x=RawOcc-CleanOcc, y=SpeciesID, fill=Ecogroup_Bottinelli))+
  geom_bar(stat = "identity")
```


Second, raw occurrences after we downloaded and cleaned them.

```{r plotOccRaw}
# load background map
world.inp <- map_data("world")

# plot total species' occurrences
plotOccRaw <- ggplot(occ_clean, 
       aes(x=longitude, y=latitude, color=datasource))+ #, alpha=`Number of Records`
  #geom_polygon(data=bg.map)
  geom_map(data = world.inp, map = world.inp, aes(map_id = region), fill = "grey80") +
  xlim(min(extent_Europe[1], na.rm = T), 40) +
  ylim(min(extent_Europe[3], na.rm = T), max(extent_Europe[4], na.rm = T)) +
  geom_point(cex=0.3)+
  scale_x_continuous(limits=c(-20, 60))+ 
  scale_y_continuous(limits=c(30,75))+
  theme_bw()+
  theme(legend.position = "bottom")+
  guides(color = guide_legend(override.aes = list(size = 3))) #makes legend icons bigger

plotOccRaw

# pdf(paste0(here::here(), "/figures/RawOccurrences_", Taxon_name, "_perDatasource.pdf")); plotOccRaw; dev.off()

rm(plotOccRaw)
```

Zoom to Germany

```{r plotOccRaw_Germany}
# plot total species' occurrences
plotOccRawGER <- ggplot(occ_clean, 
       aes(x=longitude, y=latitude, color=datasource))+ #, alpha=`Number of Records`
  #geom_polygon(data=bg.map)
  geom_point(cex=0.6)+
  scale_x_continuous(limits=c(5, 17))+ 
  scale_y_continuous(limits=c(46,57))+
  theme_bw()+
  theme(legend.position = "bottom")+
  guides(color = guide_legend(override.aes = list(size = 3))) #makes legend icons bigger

plotOccRawGER

# pdf(paste0(here::here(), "/figures/RawOccurrences_", Taxon_name, "_perDatasource_GER.pdf")); plotOccRawGER; dev.off()

rm(plotOccRawGER, occ_clean)
```


Third, we plot the data we used for the models (i.e., the gridded or rasterized data).

```{r plotOcc}
# load background map
world.inp <- map_data("world")

# calculate raw species richness
occ_rich <- occ_points %>% 
               group_by(Latitude = round(x,0), Longitude=round(y,0)) %>%
               summarise_at(vars(c(speciesNames[speciesNames$NumCells>=5,"SpeciesID"])), sum, na.rm=T)
occ_rich$Richness <- apply(occ_rich > 0, 1, sum)

# plot total species' occurrences
plotOcc <- ggplot()+ 
  geom_map(data = world.inp, map = world.inp, aes(map_id = region), fill = "grey80") +
  xlim(min(extent_Europe[1], na.rm = T), max(extent_Europe[2], na.rm = T)) +
  ylim(min(extent_Europe[3], na.rm = T), max(extent_Europe[4], na.rm = T)) +
  
  geom_point(data=occ_rich %>%
              dplyr::select(c(Latitude, Longitude, Richness)),
             aes(x=Latitude, y=Longitude, color=Richness))+ #, alpha=`Number of Species`
   
  scale_color_gradientn(    # provide any number of colors
    colors = c("black", "blue", "orange"),
    values = scales::rescale(c(1, 5, 20, 30, 50, 100, 300)), 
    breaks = c(5, 20, 50, 100, 200))+
  theme_bw()+
  theme(legend.position = "bottom", legend.text = element_text(size=8), legend.key.width = unit(2, "cm"))

plotOcc

# pdf(paste0(here::here(), "/figures/GriddedOccurrences_Richness_", Taxon_name, ".pdf")); plotOcc; dev.off()


## Alternative but wrong. Gives number of species per cell + sum of cells per Lat/long
# plotOcc <- ggplot()+ 
#   geom_map(data = world.inp, map = world.inp, aes(map_id = region), fill = "grey80") +
#   xlim(min(extent_Europe[1], na.rm = T), max(extent_Europe[2], na.rm = T)) +
#   ylim(min(extent_Europe[3], na.rm = T), max(extent_Europe[4], na.rm = T)) +
#   
#   geom_point(data=occ_points %>% 
#                     dplyr::select(c(x, y, all_of(speciesNames[speciesNames$NumCells>=5,"SpeciesID"]))) %>%
#                     group_by(x,y) %>%
#                     pivot_longer(cols = colnames(occ_points)[colnames(occ_points) %in%
#                                                                speciesNames[speciesNames$NumCells>=5,"SpeciesID"]], 
#                                  names_to = "SpeciesID", values_to = "occ",
#                                  values_drop_na = TRUE) %>%
#                     group_by(Latitude = round(x,0), Longitude = round(y,0)) %>%
#                     count(name="Number of Records"),
#              aes(x=Latitude, y=Longitude, color=`Number of Records`))+ #, alpha=`Number of Records`
#    
#   scale_color_gradientn(    # provide any number of colors
#     colors = c("black", "blue", "orange"),
#     values = scales::rescale(c(1, 5, 20, 30, 50, 100, 300)), 
#     breaks = c(5, 20, 50, 100, 200))+
#   theme_bw()+
#   theme(legend.position = "bottom", legend.text = element_text(size=8), legend.key.width = unit(2, "cm"))
# 
# plotOcc
```

Now, we make one plot with gridded occurrences per species.

```{r plotOccSpecies}
# calculate individual species' occurrences
occ_points_species <- occ_points %>% 
         pivot_longer(cols=speciesNames$SpeciesID[speciesNames$SpeciesID %in% colnames(occ_points)], 
                      names_to = "SpeciesID") %>% 
        mutate("Latitude"=round(x,0), "Longitude"=round(y,0)) %>%
        group_by(Latitude, Longitude, SpeciesID) %>%
        filter(!is.na(value)) %>%
        summarize("Number of Records"= n(), .groups="keep") %>%
        filter("Number of Records" > 0) 

# only keep species that will be analyzed (i.e., present in at least 5 grid cells)
occ_points_species <- occ_points_species[occ_points_species$SpeciesID %in%
                                           speciesNames[speciesNames$NumCells_2km >=5, "SpeciesID"],]
occ_points_species

# plot some of the individual species' occurrences
plotOccSpecies <- ggplot(occ_points_species, 
       aes(x=Latitude, y=Longitude, color=`Number of Records`, group=SpeciesID))+
  #geom_polygon(data=bg.map)
  geom_point(cex=0.015, pch=15)+
  facet_wrap(vars(SpeciesID))+
  scale_color_gradientn(    # provide any number of colors
    colors = c("black", "blue", "orange"),
    values = scales::rescale(c(1, 5, 20, 30, 50, 100, 300)), 
    breaks = c(5, 20, 50, 100, 200))+
  
  # add number of grid cells in which the species is present
  geom_text(data=occ_points_species %>% group_by(SpeciesID) %>% summarize("n"=sum(`Number of Records`)), 
             aes(x=30, y=33, label=paste0("n=", n)), color="black", 
            inherit.aes=FALSE, parse=FALSE, cex=0.7)+
  theme_bw()+
  theme(axis.text.x = element_blank(), axis.text.y = element_blank(), 
        axis.title.x = element_blank(), axis.title.y = element_blank(),
        axis.ticks.length = unit(0, "cm"),
        legend.position = "bottom", legend.text = element_text(size=5))
plotOccSpecies

# pdf(paste0(here::here(), "/figures/GriddedOccurrences_", Taxon_name, "_perSpecies.pdf")); plotOccSpecies; dev.off()

rm(plotOccSpecies, occ_points_species)
```


### Validation vs. training data

```{r plotDataValidation}
# load background map
world.inp <- map_data("world")

for(sp in unique(speciesNames[speciesNames$NumCells_2km>=5,]$SpeciesID)){ try({
    
    # read model performance evaluation table (for threshold MaxEnt & saving best model)
    mod_eval <- read.csv(file=paste0(here::here(), "/results/ModelEvaluation_", Taxon_name, "_", sp, ".csv"))
  
    best_bg <- mod_eval[mod_eval$tss+mod_eval$roc == max(mod_eval$tss+mod_eval$roc, na.rm=T) & !is.na(mod_eval$bg), "bg"]
    
    # identify and load all relevant data files
    temp.files <- list.files(path = paste0(here::here(),"/results/",Taxon_name),
                         pattern = paste0(best_bg, "[[:graph:]]*", sp), full.name = T)
    lapply(temp.files, load, .GlobalEnv)
    
    # how often do we have to run the loop? depending on number of background data simulated
    no.loop.runs <- length(temp.files)/2
    
    # empty plot list
    plot_list <- list()

    # plot data
    for(no.runs in 1:no.loop.runs){
  
      temp.files.subset <- list.files(path = paste0("./results/",Taxon_name), 
                                      pattern = paste0(best_bg, no.runs, "_[[:graph:]]*", sp), full.name = T)
      lapply(temp.files.subset, load, .GlobalEnv)
    
      plot_v <- ggplot()+
          geom_map(data = world.inp, map = world.inp, aes(map_id = region), fill = "grey80") +
          xlim(min(extent_Europe[1], na.rm = T), 40) +
          ylim(min(extent_Europe[3], na.rm = T), max(extent_Europe[4], na.rm = T)) +
          geom_point(data=validation, aes(x=x, y=y, color=as.factor(occ)), cex=0.5)+
          ggtitle(paste0("Validation data for ", sp, ": ", best_bg))+
          scale_color_manual(values=c("1"="#fde725","0"="#440154","NA"="lightgrey"))+
          theme_bw()+
          theme(axis.title = element_blank(), legend.title = element_blank(),
                legend.position = c(0.1,0.4))
      
      plot_list <- list(plot_list, plot_v)
    }
    
    # save plots
    #pdf(file=paste0(here::here(), "/figures/InputData_distribution_", Taxon_name, "_", sp, ".pdf"))
    do.call(grid.arrange, plot_list[-1])
    dev.off()
    
}, silent=T)}

```


### Summary models

We have to extract the following information per species:
* best-performing model,
* variable importance of that best model,
* background-data used,
* ...

Let's do so.

```{r}

data.frame

for(temp.sp in speciesNames$SpeciesID){ try({
  # name of the best model
  mod_eval <- read.csv(file=paste0(here::here(), "/results/ModelEvaluation_", Taxon_name, "_", spID, ".csv"))
  temp.model <- mod_eval[mod_eval$tss+mod_eval$roc == max(mod_eval$tss+mod_eval$roc, na.rm=T), "model"]
  temp.model
  
  # information about modelling (running time, background data)
  temp.details <- mod_eval[mod_eval$tss+mod_eval$roc == max(mod_eval$tss+mod_eval$roc, na.rm=T), c("model", "tss", "time", "bg")]
  temp.details
  
  # varImp of the best model
  var_imp <- read.csv(file=paste0(here::here(), "/results/", Taxon_name, "/Variable_importance_", spID, ".csv"))
  temp.vi <- var_imp[,c("Predictor", temp.model)] %>% 
    mutate("SpeciesID" = spID) %>% pivot_wider(id_cols=SpeciesID, names_from=Predictor, values_from=temp.model)
  temp.vi
  
  # merge into one data frame
  #...
  
})}
```


## Variable importance and correlations

Let's have a look at the variable importance across all models.

```{r resultVarImp}
# load variable importance data
var_imp <- read.csv(file=paste0(here::here(), "/results/", Taxon_name, "/Variable_importance_", spID, ".csv"))
var_imp

# load predictor table to get classification of variables
# load the predictor table containing the individual file names
pred_tab <- readr::read_csv(file=paste0(here::here(), "/doc/Env_Predictors_table.csv"))

# transform to long format and add variable categories
var_imp <- var_imp %>%
  pivot_longer(cols=colnames(var_imp[,-1]), names_to = "model", 
               values_to = "varImp") %>%
  left_join(pred_tab %>% dplyr::select(Predictor, Category), by="Predictor")

# add category for clay.silt
var_imp[var_imp$Predictor=="Clay.Silt","Category"] <- "Soil"

# plot VIF
plotVarImp <- ggplot(data=var_imp, aes(x=varImp, y=Predictor, fill=Category))+
  geom_bar(stat="identity")+
  xlab("Variable importance (Relative importances [0,1])")+
  facet_wrap(vars(model))+
  theme_bw()+
  theme(axis.text.y = element_text(size = 3))
plotVarImp

# pdf(paste0(here::here(), "/figures/VariableImportance_", Taxon_name, "_", spID, ".pdf")); plotVarImp; dev.off()

rm(plotVarImp, var_imp)

```


Now, let's look how the variables are correlated.

```{r resultCorrelations}
#corMatSpearman <- read.csv(file=paste0(here::here(),"/results/", Taxon_name, "/corMatSpearman_predictors.csv"))
corMatPearson <- as.matrix(read.csv(file=paste0(here::here(),"/results/", 
                                                Taxon_name, "/corMatPearson_predictors.csv")))
dimnames(corMatPearson)[[1]] <- dimnames(corMatPearson)[[2]]

# exclude based on VIF
env_vif <- read.csv(file=paste0(here::here(), "/results/", Taxon_name, "/VIF_predictors_", Taxon_name, ".csv"))

# plot correlations
#pdf(file=paste0(here::here(), "/figures/Predictor_correlation_", Taxon_name, ".pdf"))
corrplot::corrplot(corMatPearson, type = "upper", tl.cex = 0.5)
#dev.off()


#GGally::ggpairs()
```


## Individual SDMs

```{r resultSDM, include=F}
# load background map
world.inp <- map_data("world")

# load matrix containing information on number of occurrence records in grid
occ_points <- read.csv(file=paste0(here::here(), "/results/Occurrence_rasterized_1km_", Taxon_name, ".csv"))

  
for(sp in unique(speciesNames[speciesNames$NumCells_2km>=5,"SpeciesID"])){try({
  
  # read model performance evaluation table (for threshold MaxEnt & saving best model)
  mod_eval <- read.csv(file=paste0(here::here(), "/results/ModelEvaluation_", Taxon_name, "_", sp, ".csv"))

  best_bg <- mod_eval[mod_eval$tss+mod_eval$roc == max(mod_eval$tss+mod_eval$roc, na.rm=T) & !is.na(mod_eval$bg), "bg"]
  
  temp_points <- occ_points %>% dplyr::select(x,y, sp) %>% 
    mutate("occ"=occ_points[,sp]) %>% filter(!is.na(occ))
  temp_points[temp_points$occ>=1 & !is.na(temp_points$occ),"occ"] <- 1 
  
  # plot raw occurrences
  plot_occ <- ggplot()+
    geom_map(data = world.inp, map = world.inp, aes(map_id = region), fill = "grey80") +
    xlim(min(extent_Europe[1], na.rm = T), 40) +
    ylim(min(extent_Europe[3], na.rm = T), max(extent_Europe[4], na.rm = T)) +
          
    geom_point(data=temp_points, aes(x=x, y=y, color=as.factor(occ)), cex=2)+
    ggtitle(paste0("Occurrence records of ", sp))+
    scale_color_manual(values=c("dodgerblue1"))+
    #scale_color_manual(values=c("1"="#fde725","0"="#440154"))+
    theme_bw()+
    theme(axis.title = element_blank(), legend.title = element_blank(),
          legend.position = c(0.1,0.4))
  
  # plot input data (training + validation)
  load(file=paste0(here::here(), "/results/", Taxon_name, "/PA_Env_", Taxon_name, "_", sp, ".RData")) #bg.list
    
  temp_training <- bg.list[[best_bg]] %>% 
                 mutate("occ"=bg.list[[best_bg]][,1]) %>% 
                 dplyr::select(occ,x,y) #%>% filter(!is.na(occ))
  temp_training[is.na(temp_training$occ),"occ"] <- 0
  
  plot_tra <- ggplot()+
      geom_map(data = world.inp, map = world.inp, aes(map_id = region), fill = "grey80") +
      xlim(min(extent_Europe[1], na.rm = T), 40) +
      ylim(min(extent_Europe[3], na.rm = T), max(extent_Europe[4], na.rm = T)) +
    
    geom_point(data=temp_training, 
               aes(x=x, y=y, color=as.factor(occ)), cex=2)+
      ggtitle(paste0("Presence-absence data for ", sp, ": ", best_bg))+
      scale_color_manual(values=c("1"="dodgerblue1","0"="#440154"))+
      theme_bw()+
      theme(axis.title = element_blank(), legend.title = element_blank(),
            legend.position = c(0.1,0.4))
  
  
  # plot validation data (again)
  # identify and load all relevant data files
  load(paste0(here::here(), "/results/", Taxon_name, "/ValidationData_", best_bg, 1, "_", Taxon_name, "_", sp, ".RData")) #validation

  plot_val <- ggplot()+
      geom_map(data = world.inp, map = world.inp, aes(map_id = region), fill = "grey80") +
      xlim(min(extent_Europe[1], na.rm = T), 40) +
      ylim(min(extent_Europe[3], na.rm = T), max(extent_Europe[4], na.rm = T)) +
      
    geom_point(data=validation, aes(x=x, y=y, color=as.factor(occ)), cex=2)+
      ggtitle(paste0("Validation data for ", sp, ": ", best_bg))+
      scale_color_manual(values=c("1"="dodgerblue1","0"="#440154","NA"="lightgrey"))+
      theme_bw()+
      theme(axis.title = element_blank(), legend.title = element_blank(),
            legend.position = c(0.1,0.4))
  
  # load SDM
  load(paste0(here::here(), "/results/_Maps/SDM_bestPrediction_", Taxon_name, "_", sp, ".RData")) #best_pred

  plot_sdm <- ggplot(data=best_pred, aes(x=x, y=y, fill=layer))+
      geom_tile()+
      ggtitle(best_model)+
      scale_fill_viridis_c(limits = c(0,1))+
      theme_bw()+
      theme(axis.title = element_blank(), legend.title = element_blank(),
            legend.position = c(0.1,0.4))

  
  plot_list <- list(plot_occ, plot_tra, plot_val, plot_sdm)
  
  # arrange all three plots
  png(file=paste0(here::here(), "/figures/InputOutput_SDM_", Taxon_name, "_", sp, ".png"), width=2000, height=2000)
  grid.arrange(grobs = plot_list, ncol=2)
  dev.off()
  
}, silent=T)}

```


## Diversity per group


```{r resultGroupDiv, include=F}
source(file=paste0(here::here(),"Result_richness_perGroup.R"))
```

## Total diversity

```{r resultDiv, include=F}
source(file=paste0(here::here(),"Result_richness.R"))
```

## Model performance

```{r resultModelPerformance}
source(file=paste0(here::here(),"Model_performance.R"))
```

Plot model evalutation

```{r plotModelPerformance}
mod_eval <- read.csv(file=paste0(here::here(), "/results/ModelEvaluation_", Taxon_name, ".csv"))

# point plot with lables, tss over roc
pdf(paste0(here::here(), "/figures/Model_performance_", Taxon_name, "_tss-roc_perSpecies.pdf"))
ggplot(mod_eval %>% filter(!is.na(species)), aes(x=tss, y=roc, color=model))+
  geom_text(label=mod_eval[!is.na(mod_eval$species),"model"], nudge_x = 0, nudge_y = 0, check_overlap = F, cex=1)+
  facet_wrap(vars(species))+
  xlim(0,1)+
  theme_bw()
dev.off()
 
# boxplot, tss per algorithm
pdf(paste0(here::here(), "/figures/Model_performance", Taxon_name, "_boxplot.pdf"))
ggplot(mod_eval %>% filter(!is.na(species)), aes(x=tss, y=model))+
  geom_boxplot()+
  xlim(0,1)+
  theme_bw()
dev.off()

```

